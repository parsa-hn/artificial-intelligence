{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>\n",
    "    <font size=\"10\" face=\"verdana\">AI Fall 98 Project 3</font>\n",
    "</b>\n",
    "<hr>\n",
    "\n",
    "## By Parsa Hoseininejad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In our world, most of industries use different artificial intelligence methods to improve their products. One of these, is classifying texts to determine some property about them. In this project we to build a mechanism to classify different kinds of news based on a few lines of that news. We use machine learning algorithms to do this task.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Question\n",
    "To classify the news, a model should be built that gets news description, headline, and etc. and determins the type of news. To build this model, a dataset of news with their type is needed to train the model. This data set is first processed to build a dictionary. Then, we start to train our model based on that and evaluate our model based on the rest of dataset. At last, we use this model to determine news types.<br>\n",
    "The dataset used in this project is a portion of dataset prepared in the this link: https://www.kaggle.com/rmisra/news-category-dataset <br>\n",
    "It provides us with authors, category, date, headline, link and a short description of about 25,000 different news. It should be noted that in some inputs, fileds can be empty and all news type are TRAVEL, BUSINESS or STYLE & BEAUTY.<br>\n",
    "The approach used in this project is bag of words model. In this model, each word gets a probablity of appearing in a certain news type. We use these probabalities to determine news type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "<hr>\n",
    "To build a model based on dataset, baysian networks and naive bayes is used.<br>\n",
    "<img src=\"./Images/Bayes_rule.png\"/> <br>\n",
    "In the following part, each probabality is defined:<br>\n",
    "1. Posterior Probabality: The probabality of a news being class C with the condition of having words X in it.<br>\n",
    "2. Likelihood: The probabality of existance of word x in news type C. This probabality is calculated in modeling sections. For each word, the probabality would be: number of occurances/number of all words.<br>\n",
    "3. Class Prior Probabality: The probabality of a news being type c. These probabalities are extracted from the original dataset.<br>\n",
    "4. Predictor Prior Probabality: The probabality of occurance of word x1, x2, and so on in the news. This probabality is constant given the input, so it is ommited.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "To turn this data set to useful data, four major steps are done:<br>\n",
    "1. Deleting stop words: Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. These kinds of words are unimportant and they are removed to allow the model to focus on the important words instead.\n",
    "2. Turning Words into Lowercase: In the bag of words model, the position of words in the scentence doesn't matter. So, for instance, \"Style\" and \"style\" words are interpreted the same. In this case, the probabality of the word \"style\" is the sum of the probablity of \"Style\" and \"style\" which is bigger than the both probabalities. This is done to improve the accuracy of our model.\n",
    "3. Tokenizing: To tokenize the news, non alphabetic characters are used as delimiters.\n",
    "4. Lemmatization: Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For instance, \"stones\" and \"stone\" are both interpreted as \"stone\". Because these two words have the same root, it is essential to calculate the frequency of this word by adding both previous frequencies. In this project, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "After pre processing the data, first 80% of each news type is selected. Then, for every word, the occurance of it is stored in a data structure. To keep the likelihoods, 3 different dictionaries are used to store the data for each class. The likelihood of every word is calculated as mentioned before. Note that because the multiplication of numbers less than one will make smaller and smaller numbers, logarithm of each likelihood is stored to be used in further parts. We know that programming languages interpret very small integers as 0. This action will prevent this unwanted scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "This evaluating has 2 phases. In the first phase, the prediction is made between BUSINESS and TRAVEL classes. In the second phase, all classes take part in the process of evaluating.<br>\n",
    "To evaluate the model trained, the rest of the dataset is used to calculate three indicators:\n",
    "1. Recall: Recall is the fraction of the total amount of relevant instances that were actually retrieved.\n",
    "2. Precision: Precision is the fraction of relevant instances among the retrieved instances.\n",
    "3. Accuracy: Accuracy is the fraction of the total amount of relevant instances among all of the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "Oversampling in data analysis is techniques used to adjust the class distribution of a data set. In the dataset given, the distribution of the BUSINESS class is much lower than the other two classes (About 4,000 less data). So, oversampling is done by selecting radom news with BUSINESS class and appneding it to the dataset. Over sampling insures that the proababality of important words increases because there is a higher chance to select a random news containig an important word than selecting a news which dosn't contain that word. Meanwhile, the probabality of non important words decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "<hr>\n",
    "To implement this model, the data is read and stored in a dataframe available in <code>Pandas</code> library. The tokenizing and lemmatization and stop word deletion is implemented using <code>NLTK</code> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import random \n",
    "\n",
    "TRAVEL = 'TRAVEL'\n",
    "BUSINESS = 'BUSINESS'\n",
    "STYLE = 'STYLE & BEAUTY'\n",
    "DESCRIPTION_COL = 'short_description'\n",
    "CATEGORY_COL = 'category'\n",
    "HEADLINE_COL = 'headline'\n",
    "WORDS_COL = 'keywords'\n",
    "\n",
    "businessP = math.log(5937 / 200853, 10) \n",
    "travelP = math.log(9887 / 200853, 10)\n",
    "styleP = math.log(9649 / 200853, 10)\n",
    "trainPercentage = 0.8\n",
    "\n",
    "stopWords = set(stopwords.words('english')) \n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, dataFileAddr, testFileAddr):\n",
    "        self.dataFileAddr = dataFileAddr\n",
    "        self.testFileAddr = testFileAddr\n",
    "        self.travelDict = {}\n",
    "        self.styleDict = {}\n",
    "        self.businessDict = {}\n",
    "\n",
    "    def run(self):\n",
    "        self.data = self.cleanData(self.dataFileAddr)\n",
    "        self.oversample()\n",
    "        self.train()\n",
    "\n",
    "    def cleanData(self, path):\n",
    "        df = pd.read_csv(path, delimiter = ',')\n",
    "        df.drop(['index', 'authors', 'date', 'link'], axis=1, inplace=True)\n",
    "        df.dropna(how='all', inplace=True)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        wordsCol = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            description = ''\n",
    "            if not isinstance(row[HEADLINE_COL], float):\n",
    "                desciption = row[HEADLINE_COL]\n",
    "            if not isinstance(row[DESCRIPTION_COL], float):\n",
    "                desciption += ' ' + row[DESCRIPTION_COL]\n",
    "            words = tokenizer.tokenize(desciption)\n",
    "            filteredWords = [lemmatizer.lemmatize(w.lower()) for w in words if not w in stopWords]\n",
    "            wordsCol.append(filteredWords)\n",
    "        \n",
    "        df.drop([DESCRIPTION_COL, HEADLINE_COL], axis=1, inplace=True)\n",
    "        df[WORDS_COL] = wordsCol\n",
    "        return df\n",
    "\n",
    "    def oversample(self):\n",
    "        travelDf = self.data.loc[self.data[CATEGORY_COL] == TRAVEL]\n",
    "        businessDf = self.data.loc[self.data[CATEGORY_COL] == BUSINESS]\n",
    "        businessTrainDf = businessDf.head(int(trainPercentage * len(businessDf)))\n",
    "\n",
    "        randomSelected = []\n",
    "\n",
    "        for i in range(len(travelDf) - len(businessDf)):\n",
    "            idx = int(random.randint(0, len(businessTrainDf)-1))\n",
    "            for index, row in businessTrainDf.iloc[[idx]].iterrows():\n",
    "                randomSelected.insert(0, {CATEGORY_COL: row[CATEGORY_COL], WORDS_COL: row[WORDS_COL]})\n",
    "        self.data = pd.concat([pd.DataFrame(randomSelected), self.data], ignore_index=True)\n",
    "\n",
    "    def train(self):\n",
    "        travelDf = self.data.loc[self.data[CATEGORY_COL] == TRAVEL]\n",
    "        businessDf = self.data.loc[self.data[CATEGORY_COL] == BUSINESS]\n",
    "        styleDf = self.data.loc[self.data[CATEGORY_COL] == STYLE]\n",
    "\n",
    "        wordsCount = 0\n",
    "        lastTravelTrainIdx = int(trainPercentage * len(travelDf))\n",
    "        idx = 0\n",
    "        for index, row in travelDf.iterrows():\n",
    "            for word in row[WORDS_COL]:\n",
    "                wordsCount += 1\n",
    "                if word in self.travelDict:\n",
    "                    self.travelDict[word] += 1\n",
    "                else:\n",
    "                    self.travelDict[word] = 1\n",
    "            idx += 1\n",
    "            if idx > lastTravelTrainIdx:\n",
    "                break\n",
    "        for key, value in self.travelDict.items():\n",
    "            self.travelDict[key] = math.log(value/wordsCount, 10)\n",
    "    \n",
    "        wordsCount = 0\n",
    "        lastBusinessTrainIdx = int(trainPercentage * len(businessDf))\n",
    "        idx = 0\n",
    "        for index, row in businessDf.iterrows():\n",
    "            for word in row[WORDS_COL]:\n",
    "                wordsCount += 1\n",
    "                if word in self.businessDict:\n",
    "                    self.businessDict[word] += 1\n",
    "                else:\n",
    "                    self.businessDict[word] = 1\n",
    "            idx += 1\n",
    "            if idx > lastBusinessTrainIdx:\n",
    "                break\n",
    "        for key, value in self.businessDict.items():\n",
    "            self.businessDict[key] = math.log(value/wordsCount, 10)\n",
    "\n",
    "        wordsCount = 0\n",
    "        lastStyleTrainIdx = int(trainPercentage * len(styleDf))\n",
    "        idx = 0\n",
    "        for index, row in styleDf.iterrows():\n",
    "            for word in row[WORDS_COL]:\n",
    "                wordsCount += 1\n",
    "                if word in self.styleDict:\n",
    "                    self.styleDict[word] += 1\n",
    "                else:\n",
    "                    self.styleDict[word] = 1\n",
    "            idx += 1\n",
    "            if idx > lastStyleTrainIdx:\n",
    "                break\n",
    "        for key, value in self.styleDict.items():\n",
    "            self.styleDict[key] = math.log(value/wordsCount, 10)\n",
    "\n",
    "    def evaluatePhase1(self):\n",
    "        travelDf = self.data.loc[self.data[CATEGORY_COL] == TRAVEL]\n",
    "        businessDf = self.data.loc[self.data[CATEGORY_COL] == BUSINESS]\n",
    "        travelDf = travelDf.tail(int((1-trainPercentage) * len(travelDf)))\n",
    "        businessDf = businessDf.tail(int((1-trainPercentage) * len(businessDf)))\n",
    "        \n",
    "        currectTravel = 0\n",
    "        wrongTravel = 0\n",
    "        for index, row in travelDf.iterrows():\n",
    "            if (self.getNewsTypePhase1(row[WORDS_COL]) == TRAVEL): \n",
    "                currectTravel += 1\n",
    "            else:\n",
    "                wrongTravel += 1\n",
    "        travelRecall = currectTravel / len(travelDf) * 100\n",
    "\n",
    "        currectBusiness = 0\n",
    "        wrongBusiness = 0\n",
    "        for index, row in businessDf.iterrows():\n",
    "            if (self.getNewsTypePhase1(row[WORDS_COL]) == BUSINESS): \n",
    "                currectBusiness += 1\n",
    "            else:\n",
    "                wrongBusiness += 1\n",
    "        businessRecall = currectBusiness / len(businessDf) * 100\n",
    "\n",
    "        travelPrecision = currectTravel / (currectTravel+wrongBusiness) * 100\n",
    "        businessPrecision = currectBusiness / (currectBusiness+wrongTravel) * 100\n",
    "\n",
    "        accuracy = (currectBusiness + currectTravel) / (len(travelDf) + len(businessDf)) * 100\n",
    "\n",
    "        outDf = pd.DataFrame(columns=['Phase1', 'Travel', 'Business'])\n",
    "        outDf['Phase1'] = ['Recall', 'Precision', 'Accuracy']\n",
    "        outDf['Travel'] = [travelRecall, travelPrecision, accuracy]\n",
    "        outDf['Business'] = [businessRecall, businessPrecision, accuracy]\n",
    "        return outDf\n",
    "            \n",
    "    def evaluatePhase2(self):\n",
    "        travelDf = self.data.loc[self.data[CATEGORY_COL] == TRAVEL]\n",
    "        businessDf = self.data.loc[self.data[CATEGORY_COL] == BUSINESS]\n",
    "        styleDf = self.data.loc[self.data[CATEGORY_COL] == STYLE]\n",
    "        travelDf = travelDf.tail(int((1-trainPercentage) * len(travelDf)))\n",
    "        businessDf = businessDf.tail(int((1-trainPercentage) * len(businessDf)))\n",
    "        styleDf = styleDf.tail(int((1-trainPercentage) * len(styleDf)))\n",
    "        \n",
    "        travelCount = 0\n",
    "        businessCount = 0\n",
    "        styleCount = 0        \n",
    "        currectTravel = 0\n",
    "        for index, row in travelDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL): \n",
    "                currectTravel += 1\n",
    "                travelCount += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == BUSINESS):\n",
    "                businessCount += 1\n",
    "            else:\n",
    "                styleCount += 1\n",
    "        travelRecall = currectTravel / len(travelDf) * 100\n",
    "\n",
    "        currectBusiness = 0\n",
    "        for index, row in businessDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == BUSINESS): \n",
    "                currectBusiness += 1\n",
    "                businessCount += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL):\n",
    "                travelCount += 1\n",
    "            else:\n",
    "                styleCount += 1\n",
    "        businessRecall = currectBusiness / len(businessDf) * 100\n",
    "\n",
    "        currectStyle = 0\n",
    "        for index, row in styleDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == STYLE): \n",
    "                currectStyle += 1\n",
    "                styleCount += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL):\n",
    "                travelCount += 1\n",
    "            else:\n",
    "                businessCount += 1\n",
    "        styleRecall = currectStyle / len(styleDf) * 100\n",
    "\n",
    "        travelPrecision = currectTravel / travelCount * 100\n",
    "        businessPrecision = currectBusiness / businessCount * 100\n",
    "        stylePrecision = currectStyle / styleCount * 100\n",
    "\n",
    "        accuracy = (currectBusiness + currectTravel + currectStyle) / (len(travelDf) + len(businessDf) + len(styleDf)) * 100\n",
    "\n",
    "        outDf = pd.DataFrame(columns=['Phase1', 'Travel', 'Business', 'Style & Beauty'])\n",
    "        outDf['Phase1'] = ['Recall', 'Precision', 'Accuracy']\n",
    "        outDf['Travel'] = [travelRecall, travelPrecision, accuracy]\n",
    "        outDf['Business'] = [businessRecall, businessPrecision, accuracy]\n",
    "        outDf['Style & Beauty'] = [styleRecall, stylePrecision, accuracy]\n",
    "        return outDf\n",
    "    \n",
    "    def getNewsTypePhase1(self, words):\n",
    "        if self.travelProbability(words) > self.businessProbablity(words):\n",
    "            return TRAVEL\n",
    "        else:\n",
    "            return BUSINESS\n",
    "\n",
    "    def getNewsTypePhase2(self, words):\n",
    "        t = self.travelProbability(words)\n",
    "        b = self.businessProbablity(words)\n",
    "        s = self.styleProbablity(words)\n",
    "        if t > b and t > s:\n",
    "            return TRAVEL\n",
    "        elif b > s:\n",
    "            return BUSINESS\n",
    "        else:\n",
    "            return STYLE\n",
    "    \n",
    "    def travelProbability(self, words):\n",
    "        p = travelP\n",
    "        for word in words:\n",
    "            if word in self.travelDict:\n",
    "                p += self.travelDict[word]\n",
    "            else:\n",
    "                p -= 6\n",
    "        return p\n",
    "\n",
    "    def businessProbablity(self, words):\n",
    "        p = businessP\n",
    "        for word in words:\n",
    "            if word in self.businessDict:\n",
    "                p += self.businessDict[word]\n",
    "            else:\n",
    "                p -= 6\n",
    "        return p\n",
    "\n",
    "    def styleProbablity(self, words):\n",
    "        p = styleP\n",
    "        for word in words:\n",
    "            if word in self.styleDict:\n",
    "                p += self.styleDict[word]\n",
    "            else:\n",
    "                p -= 6\n",
    "        return p\n",
    "            \n",
    "    def classify(self):\n",
    "        self.test = self.cleanData(self.testFileAddr)\n",
    "        file = open('output.csv', 'w')\n",
    "        file.write('index,category\\n')\n",
    "        for index, row in self.test.iterrows():\n",
    "            file.write(str(index) + ',' + self.getNewsTypePhase2(row[WORDS_COL]) + '\\n')\n",
    "        file.close()\n",
    "        \n",
    "    def getConfusionMatrix(self):\n",
    "        travelDf = self.data.loc[self.data[CATEGORY_COL] == TRAVEL]\n",
    "        businessDf = self.data.loc[self.data[CATEGORY_COL] == BUSINESS]\n",
    "        styleDf = self.data.loc[self.data[CATEGORY_COL] == STYLE]\n",
    "        travelDf = travelDf.tail(int((1-trainPercentage) * len(travelDf)))\n",
    "        businessDf = businessDf.tail(int((1-trainPercentage) * len(businessDf)))\n",
    "        styleDf = styleDf.tail(int((1-trainPercentage) * len(styleDf)))\n",
    "\n",
    "        travelPredictedStyle = 0\n",
    "        travelPredictedBusiness = 0\n",
    "        businessPredictedStyle = 0\n",
    "        businessPredictedTravel = 0\n",
    "        stylePredictedTravel = 0\n",
    "        stylePredictedBusiness = 0\n",
    "\n",
    "        currectTravel = 0\n",
    "        for index, row in travelDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL): \n",
    "                currectTravel += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == BUSINESS):\n",
    "                travelPredictedBusiness += 1\n",
    "            else:\n",
    "                travelPredictedStyle += 1\n",
    "\n",
    "        currectBusiness = 0\n",
    "        for index, row in businessDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == BUSINESS): \n",
    "                currectBusiness += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL):\n",
    "                businessPredictedTravel += 1\n",
    "            else:\n",
    "                businessPredictedStyle += 1\n",
    "\n",
    "        currectStyle = 0\n",
    "        for index, row in styleDf.iterrows():\n",
    "            if (self.getNewsTypePhase2(row[WORDS_COL]) == STYLE): \n",
    "                currectStyle += 1\n",
    "            elif (self.getNewsTypePhase2(row[WORDS_COL]) == TRAVEL):\n",
    "                stylePredictedTravel += 1\n",
    "            else:\n",
    "                stylePredictedBusiness += 1\n",
    "\n",
    "        outDf = pd.DataFrame(columns=['x', 'Predicted: Travel', 'Predicted: Business', 'Predicted: Style & Beauty'])\n",
    "        outDf['x'] = ['Actual: Travel', 'Actual: Business', 'Actual: Style & Beauty']\n",
    "        outDf['Predicted: Travel'] = [currectTravel, businessPredictedTravel, stylePredictedTravel]\n",
    "        outDf['Predicted: Business'] = [travelPredictedBusiness, currectBusiness, stylePredictedBusiness]\n",
    "        outDf['Predicted: Style & Beauty'] = [travelPredictedStyle, businessPredictedStyle, currectStyle]\n",
    "        return outDf\n",
    "\n",
    "        \n",
    "        \n",
    "cl = Classifier('./Attachment/data.csv', './Attachment/test.csv')\n",
    "cl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase1</th>\n",
       "      <th>Travel</th>\n",
       "      <th>Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall</td>\n",
       "      <td>95.559303</td>\n",
       "      <td>92.636313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>92.845440</td>\n",
       "      <td>95.425594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>94.097808</td>\n",
       "      <td>94.097808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Phase1     Travel   Business\n",
       "0     Recall  95.559303  92.636313\n",
       "1  Precision  92.845440  95.425594\n",
       "2   Accuracy  94.097808  94.097808"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.evaluatePhase1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase1</th>\n",
       "      <th>Travel</th>\n",
       "      <th>Business</th>\n",
       "      <th>Style &amp; Beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recall</td>\n",
       "      <td>93.816751</td>\n",
       "      <td>90.949972</td>\n",
       "      <td>93.260369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>89.827772</td>\n",
       "      <td>92.881745</td>\n",
       "      <td>95.572609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>92.670948</td>\n",
       "      <td>92.670948</td>\n",
       "      <td>92.670948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Phase1     Travel   Business  Style & Beauty\n",
       "0     Recall  93.816751  90.949972       93.260369\n",
       "1  Precision  89.827772  92.881745       95.572609\n",
       "2   Accuracy  92.670948  92.670948       92.670948"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.evaluatePhase2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The number of correct and incorrect predictions are summarized with count values and broken down by each class. The confusion matrix of this model is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>Predicted: Travel</th>\n",
       "      <th>Predicted: Business</th>\n",
       "      <th>Predicted: Style &amp; Beauty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actual: Travel</td>\n",
       "      <td>1669</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actual: Business</td>\n",
       "      <td>118</td>\n",
       "      <td>1618</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actual: Style &amp; Beauty</td>\n",
       "      <td>71</td>\n",
       "      <td>46</td>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        x  Predicted: Travel  Predicted: Business  \\\n",
       "0          Actual: Travel               1669                   78   \n",
       "1        Actual: Business                118                 1618   \n",
       "2  Actual: Style & Beauty                 71                   46   \n",
       "\n",
       "   Predicted: Style & Beauty  \n",
       "0                         32  \n",
       "1                         43  \n",
       "2                       1619  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.getConfusionMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. __Stemming or Lemmatization?__ Lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words. In this case, lemmatization improves all of the indicator by less than 1%.\n",
    "2. __What is tf-idf measure?__ TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents. This measure is calculated by this formula: __tf-idf(t, d) = tf(t, d)* idf(t, d)__ for a term t in document d. <code>tf(t, d)</code> is basically our measure in this project. <code>idf(t, d)</code> is calculated by this formula: idf(t) = N/ df(t) where df(t) = Document frequency of a term t and N(t) = Number of documents containing the term t. If this measure was selected, the tf-idf should have been calculated for each word in each class. Then, the rest of the operations would be the same.\n",
    "3. __What if a model only consider precision?__ For example, consider a model which takes an email and determines if it's spam or ham. If the model is trained so that it takes so many factors for determining if an email is spam, the precision will be 100%. Howerver, it won't determine an email spam if there is a good chance that the email is spam. In this case, the precision would be high but the model doesn't work correctly.\n",
    "4. __What happens if a word is occured in only one class?__ In this case, the probabality implied by that word would be 10^-6 for those classes which doesn't contain the word. So the model will probably implies the class containing the word as answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
